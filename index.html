<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Meng CHU | HKUST PhD Student</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Instrument+Serif:ital@0;1&family=JetBrains+Mono:wght@400;500&family=Outfit:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Animated Background -->
    <div class="bg-gradient"></div>
    <div class="bg-grid"></div>

    <!-- Navigation -->
    <nav class="nav">
        <div class="nav-container">
            <a href="#" class="nav-logo">MC</a>
            <div class="nav-links">
                <a href="#about">About</a>
                <a href="#publications">Publications</a>
                <a href="#experience">Experience</a>
                <a href="#education">Education</a>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="hero" id="about">
        <div class="hero-content">
            <div class="hero-image">
                <img src="images/profile.jpeg" alt="Meng CHU" onerror="this.src='https://via.placeholder.com/200?text=MC'">
                <div class="hero-image-ring"></div>
            </div>
            <div class="hero-text">
                <h1 class="hero-title">
                    <span class="hero-name">Meng CHU</span>
                    <span class="hero-name-cn">ÂÇ® Ëêå</span>
                </h1>
                <p class="hero-subtitle">Ph.D. Student in Computer Science</p>
                <p class="hero-affiliation">
                    <a href="https://www.cse.ust.hk/" target="_blank">Hong Kong University of Science and Technology</a>
                </p>
                <p class="hero-bio">
                    I study <strong>Multimodal Learning</strong>, <strong>Agentic AI</strong>, and <strong>Large Language Models</strong>. 
                    Currently advised by <a href="https://jiaya.me/" target="_blank">Prof. Jiaya Jia</a>.
                </p>
                <div class="hero-links">
                    <a href="mailto:richardtotrueman@gmail.com" class="link-btn" title="Email">
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
                        <span>Email</span>
                    </a>
                    <a href="https://scholar.google.com/citations?user=D-CKSJAAAAAJ" target="_blank" class="link-btn" title="Google Scholar">
                        <svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"/></svg>
                        <span>Scholar</span>
                    </a>
                    <a href="https://github.com/TruemanV5" target="_blank" class="link-btn" title="GitHub">
                        <svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                        <span>GitHub</span>
                    </a>
                    <a href="https://linkedin.com/in/richard-chu-3698241a0" target="_blank" class="link-btn" title="LinkedIn">
                        <svg viewBox="0 0 24 24" fill="currentColor"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
                        <span>LinkedIn</span>
                    </a>
                </div>
            </div>
        </div>
    </header>

    <!-- Research Interests -->
    <section class="interests">
        <div class="container">
            <div class="interest-tags">
                <span class="tag">Multimodal Learning</span>
                <span class="tag">Agentic AI</span>
                <span class="tag">Large Language Models</span>
                <span class="tag">Reinforcement Learning</span>
                <span class="tag">World Models</span>
            </div>
        </div>
    </section>

    <!-- News Section -->
    <section class="news">
        <div class="container">
            <h2 class="section-title">
                <span class="title-icon">üì¢</span>
                News
            </h2>
            <ul class="news-list">
                <li><span class="news-date">2026.01</span> One paper accepted to <strong>AAAI 2026</strong> as Oral Presentation!</li>
                <li><span class="news-date">2025.07</span> One paper accepted to <strong>ICCV 2025</strong>!</li>
                <li><span class="news-date">2025.04</span> One paper accepted to <strong>ACM MM 2025</strong>!</li>
                <li><span class="news-date">2024.09</span> Started my Ph.D. journey at <strong>HKUST</strong>!</li>
                <li><span class="news-date">2024.07</span> One paper accepted to <strong>ECCV 2024</strong>!</li>
            </ul>
        </div>
    </section>

    <!-- Publications Section -->
    <section class="publications" id="publications">
        <div class="container">
            <h2 class="section-title">
                <span class="title-icon">üìö</span>
                Selected Publications
            </h2>

            <!-- Publication 1 -->
            <article class="pub-card">
                <div class="pub-venue-badge venue-aaai">AAAI 2026 Oral</div>
                <div class="pub-content">
                    <h3 class="pub-title">TraveLLaMA: Facilitating Multimodal Large Language Models to Understand Urban Scenes and Provide Travel Assistance</h3>
                    <p class="pub-authors">
                        <strong>Meng Chu</strong>, Yukang Chen, Haoxuan Gui, Shaoyuan Yu, Yi Wang, Jiaya Jia
                    </p>
                    <p class="pub-desc">
                        Built a large-scale multimodal travel dataset with 220,000 QA pairs covering 30,000 locations across three continents. Developed a two-phase data construction pipeline for urban scene understanding.
                    </p>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2504.16505" target="_blank" class="pub-link">üìÑ Paper</a>
                        <a href="#" class="pub-link">üíª Code</a>
                    </div>
                </div>
            </article>

            <!-- Publication 2 -->
            <article class="pub-card">
                <div class="pub-venue-badge venue-iccv">ICCV 2025</div>
                <div class="pub-content">
                    <h3 class="pub-title">VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos</h3>
                    <p class="pub-authors">
                        Jiaheng Yu*, Yinuo Wu*, <strong>Meng Chu*</strong>, et al.
                    </p>
                    <p class="pub-desc">
                        Constructed VRBench with 1,010 narrative videos emphasizing plot-driven reasoning. Designed a human‚ÄìAI collaborative annotation pipeline with temporally grounded QA pairs.
                    </p>
                    <div class="pub-links">
                        <a href="#" target="_blank" class="pub-link">üìÑ Paper</a>
                        <a href="#" class="pub-link">üåê Project</a>
                    </div>
                </div>
            </article>

            <!-- Publication 3 -->
            <article class="pub-card">
                <div class="pub-venue-badge venue-acmmm">ACM MM 2025</div>
                <div class="pub-content">
                    <h3 class="pub-title">GraphVideoAgent: Understanding Long Videos via LLM-Powered Entity Relation Graphs</h3>
                    <p class="pub-authors">
                        <strong>Meng Chu</strong>, Yicong Li, Tat-Seng Chua
                    </p>
                    <p class="pub-desc">
                        Proposed a graph-based memory agent that tracks entities and relations across long videos. Achieved 56.3%/73.3% accuracy while analyzing only 8.2 frames on average.
                    </p>
                    <div class="pub-links">
                        <a href="https://arxiv.org/pdf/2501.15953" target="_blank" class="pub-link">üìÑ Paper</a>
                        <a href="https://doi.org/10.1145/3746027.3755537" target="_blank" class="pub-link">üîó DOI</a>
                    </div>
                </div>
            </article>

            <!-- Publication 4 -->
            <article class="pub-card">
                <div class="pub-venue-badge venue-eccv">ECCV 2024</div>
                <div class="pub-content">
                    <h3 class="pub-title">Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching</h3>
                    <p class="pub-authors">
                        <strong>Meng Chu</strong>, Zhedong Zheng, Wenhao Ji, Tao Wang, Tat-Seng Chua
                    </p>
                    <p class="pub-desc">
                        Constructed GeoText-1652, an image‚Äìtext‚Äìbounding box benchmark for drone-view vision-language understanding. Proposed a spatial-aware matching method achieving 31.2% Recall@10.
                    </p>
                    <div class="pub-links">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-73247-8_13" target="_blank" class="pub-link">üìÑ Paper</a>
                        <a href="https://github.com/MultimodalGeo/GeoText-1652" target="_blank" class="pub-link">üíª Code</a>
                        <a href="https://multimodalgeo.github.io/GeoText/" target="_blank" class="pub-link">üåê Project</a>
                        <a href="https://huggingface.co/datasets/truemanv5666/GeoText1652_Dataset" target="_blank" class="pub-link">ü§ó Dataset</a>
                    </div>
                </div>
            </article>

            <!-- Publication 5 -->
            <article class="pub-card">
                <div class="pub-venue-badge venue-journal">IEEE IoT Journal 2022</div>
                <div class="pub-content">
                    <h3 class="pub-title">Multisensory Fusion, Haptic and Visual Feedback Based Teleoperation System Under IoT Framework</h3>
                    <p class="pub-authors">
                        <strong>Meng Chu</strong>, Zhi Cui, Ailing Zhang, Jialei Yao, Chenyu Tang, Zhe Fu, Arokia Nathan, Shuo Gao
                    </p>
                    <p class="pub-desc">
                        Developed a teleoperation system integrating multisensory fusion with haptic and visual feedback under IoT framework. Published in IEEE Internet of Things Journal (IF=8.2).
                    </p>
                    <div class="pub-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/9758071" target="_blank" class="pub-link">üìÑ Paper</a>
                    </div>
                </div>
            </article>

            <!-- Preprint -->
            <article class="pub-card pub-preprint">
                <div class="pub-venue-badge venue-preprint">Preprint</div>
                <div class="pub-content">
                    <h3 class="pub-title">VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis</h3>
                    <p class="pub-authors">
                        <strong>Meng Chu</strong>, Senqiao Yang, Haoxuan Che, et al., Jiaya Jia
                    </p>
                    <p class="pub-desc">
                        Proposed LGBench with 2,000 tasks and 29,000+ structured goals. Developed VisionDirector, a training-free VLM-driven closed-loop framework. Applied GRPO to reduce editing rounds by 26%.
                    </p>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2512.19243" target="_blank" class="pub-link">üìÑ arXiv</a>
                    </div>
                </div>
            </article>

        </div>
    </section>

    <!-- Experience Section -->
    <section class="experience" id="experience">
        <div class="container">
            <h2 class="section-title">
                <span class="title-icon">üíº</span>
                Experience
            </h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-marker"></div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Research Intern</h3>
                            <span class="timeline-date">Sep 2024 ‚Äì Apr 2025</span>
                        </div>
                        <p class="timeline-org">OpenGVLab, Shanghai AI Laboratory</p>
                        <p class="timeline-desc">Mentors: Dr. Yi Wang, Prof. Limin Wang</p>
                        <p class="timeline-details">Worked on VRBench for multi-step reasoning in long narrative videos.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-marker"></div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Research Intern</h3>
                            <span class="timeline-date">Dec 2022 ‚Äì Jul 2024</span>
                        </div>
                        <p class="timeline-org">NeXT++ Research Center, National University of Singapore</p>
                        <p class="timeline-desc">Mentors: Dr. Zhedong Zheng, Dr. Yicong Li</p>
                        <p class="timeline-details">Developed GeoText-1652 benchmark and GraphVideoAgent for long-form video understanding.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Education Section -->
    <section class="education" id="education">
        <div class="container">
            <h2 class="section-title">
                <span class="title-icon">üéì</span>
                Education
            </h2>
            <div class="edu-grid">
                <div class="edu-card">
                    <div class="edu-logo">üá≠üá∞</div>
                    <div class="edu-info">
                        <h3>Hong Kong University of Science and Technology</h3>
                        <p class="edu-degree">Ph.D. in Computer Science and Engineering</p>
                        <p class="edu-date">Sep 2024 ‚Äì Present</p>
                        <p class="edu-advisor">Advisor: Prof. Jiaya Jia</p>
                    </div>
                </div>
                <div class="edu-card">
                    <div class="edu-logo">üá∏üá¨</div>
                    <div class="edu-info">
                        <h3>National University of Singapore</h3>
                        <p class="edu-degree">M.Comp. in Artificial Intelligence</p>
                        <p class="edu-date">Aug 2022 ‚Äì Jun 2024</p>
                        <p class="edu-honors">Honours: Distinction | GPA: 4.27/5.0 | Dean's List</p>
                        <p class="edu-advisor">Advisors: Prof. Tat-Seng Chua, Prof. Zhedong Zheng</p>
                    </div>
                </div>
                <div class="edu-card">
                    <div class="edu-logo">üá®üá≥</div>
                    <div class="edu-info">
                        <h3>Beihang University (BUAA)</h3>
                        <p class="edu-degree">B.Eng. in Instrumentation and Optoelectronic Engineering</p>
                        <p class="edu-date">Aug 2018 ‚Äì Jun 2022</p>
                        <p class="edu-honors">GPA: 3.6/4.0 | Top 0.3% in Gaokao</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <p>¬© 2026 Meng CHU. Built with ‚ù§Ô∏è</p>
                <p class="footer-update">Last updated: January 2026</p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>

