---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

Hi, this is Meng CHU (Ë§öËíô)'s website!  
I am a second-year Ph.D. student at [Hong Kong University of Science and Technology (HKUST)](https://www.cse.ust.hk/), advised by [Prof. Jiaya Jia](https://jiaya.me/home).  
Before that, I obtained my Master's degree from [National University of Singapore (NUS)](https://www.comp.nus.edu.sg/) with Distinction, supervised by [Prof. Tat-Seng Chua](https://www.chuatatseng.com/) and [Prof. Zhedong Zheng](https://www.zdzheng.xyz/).

My research focuses on **Multimodal Learning**, **Agentic AI**, and **Large Language Models**.  
I am passionate about building intelligent systems that can understand and interact with the world.

If you are interested in collaboration, please feel free to contact me via [Email](mailto:truemanv5666@gmail.com).


# üî• News
- *2026.01*: &nbsp;üéâüéâ One paper accepted to **AAAI 2026** as **Oral** Presentation!
- *2025.07*: &nbsp;üéâüéâ One paper accepted to **ICCV 2025**!
- *2025.04*: &nbsp;üéâüéâ One paper accepted to **ACM MM 2025**!
- *2024.09*: &nbsp;üéâüéâ Started my Ph.D. journey at **HKUST**!
- *2024.07*: &nbsp;üéâüéâ One paper accepted to **ECCV 2024**!


# üìù Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2026 Oral</div><img src='images/tafs.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**TraveLLaMA: A Multimodal Travel Assistant with Large-Scale Dataset and Structured Reasoning**](https://arxiv.org/abs/2504.16505)
<div style="display: inline">
    <a href="https://arxiv.org/abs/2504.16505"> <strong>[Paper]</strong></a>
    <a href="https://travellama-best.github.io/"> <strong>[Project]</strong></a>
    <a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()" ><strong>[Abstract]</strong></a>
    <div class="abstract" style="overflow: hidden; display: none;">  
        <p> We present TraveLLaMA, a specialized multimodal language model for comprehensive travel assistance. Our contributions include: (1) TravelQA, a novel dataset of 265k QA pairs combining 160k text QA, 100k vision-language QA, and 5k expert-annotated CoT reasoning examples; (2) Travel-CoT, a structured reasoning framework that decomposes travel queries into spatial, temporal, and practical dimensions, improving accuracy by 10.8%; and (3) an interactive agent system achieving a SUS score of 82.5 in user studies with 500 participants. </p>
    </div>
</div>

**Meng Chu**, Yukang Chen, Haokun Gui, Shaozuo Yu, Yi Wang, Jiaya Jia

- **TravelQA Dataset** - **265K** QA pairs across **35+** cities worldwide.
- **Travel-CoT Reasoning** - **10.8%** accuracy improvement with structured reasoning.
- **User Study** - SUS score of **82.5** (Excellent) with 500 participants.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/website.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos**](https://arxiv.org/abs/2506.10857)
<div style="display: inline">
    <a href="https://arxiv.org/abs/2506.10857"> <strong>[Paper]</strong></a>
    <a href="https://vrbench.github.io/"> <strong>[Project]</strong></a>
    <a href="https://github.com/OpenGVLab/VRBench"> <strong>[Code]</strong></a>
    <a href="https://huggingface.co/datasets/OpenGVLab/VRBench"> <strong>[Dataset]</strong></a>
    <a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()" ><strong>[Abstract]</strong></a>
    <div class="abstract" style="overflow: hidden; display: none;">  
        <p> We present VRBench, the first long narrative video benchmark for evaluating multi-step reasoning capabilities. It comprises 960 long videos (avg. 1.6 hours), 8,243 human-labeled multi-step QA pairs, and 25,106 reasoning steps with timestamps. We develop a human-AI collaborative framework generating coherent reasoning chains spanning seven types. VRBench designs a multi-phase evaluation pipeline assessing models at both outcome and process levels. </p>
    </div>
<img src='https://img.shields.io/github/stars/OpenGVLab/VRBench.svg?style=social&label=Star' alt="VRBench" height="100%">
</div>

Jiashuo Yu\*, Yue Wu\*, **Meng Chu\***, Zhifei Ren\*, Zizheng Huang\*, et al.

- **Long Narrative Videos** - **960** videos with avg. **1.6 hours** duration.
- **Multi-step Reasoning** - **8,243** QA pairs with **25,106** reasoning steps.
- **Multi-phase Evaluation** - Outcome and process level assessment.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2025</div><img src='images/website.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**GraphVideoAgent: Understanding Long Videos via LLM-Powered Entity Relation Graphs**](https://arxiv.org/pdf/2501.15953)
<div style="display: inline">
    <a href="https://arxiv.org/pdf/2501.15953"> <strong>[Paper]</strong></a>
    <a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()" ><strong>[Abstract]</strong></a>
    <div class="abstract" style="overflow: hidden; display: none;">  
        <p> Proposed a graph-based memory agent that tracks entities and relations across long videos. Developed a temporal reasoning framework achieving 56.3% / 73.3% accuracy while analyzing only 8.2 frames on average, significantly improving efficiency. </p>
    </div>
</div>

**Meng Chu**, Yicong Li, Tat-Seng Chua

- **Graph-based Memory** - Track entities and relations across long videos.
- **Efficient Reasoning** - **56.3%/73.3%** accuracy with only **8.2 frames** on average.
- **Temporal Understanding** - Long-form video comprehension.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/geotext.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching**](https://link.springer.com/chapter/10.1007/978-3-031-73247-8_13)
<div style="display: inline">
    <a href="https://link.springer.com/chapter/10.1007/978-3-031-73247-8_13"> <strong>[Paper]</strong></a>
    <a href="https://github.com/MultimodalGeo/GeoText-1652"> <strong>[Code]</strong></a>
    <a href="https://multimodalgeo.github.io/GeoText/"> <strong>[Project]</strong></a>
    <a href="https://huggingface.co/datasets/truemanv5666/GeoText1652_Dataset"> <strong>[Dataset]</strong></a>
    <a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()" ><strong>[Abstract]</strong></a>
    <div class="abstract" style="overflow: hidden; display: none;">  
        <p> Constructed GeoText-1652, an image‚Äìtext‚Äìbounding box benchmark for drone-view vision-language understanding. Proposed a spatial-aware image‚Äìtext matching method, outperforming ALBEF and X-VLM. Achieved a 31.2% Recall@10, a significant improvement over prior baselines. </p>
    </div>
<img src='https://img.shields.io/github/stars/MultimodalGeo/GeoText-1652.svg?style=social&label=Star' alt="GeoText" height="100%">
</div>

**Meng Chu**, Zhedong Zheng, Wenhao Ji, Tao Wang, Tat-Seng Chua

- **Drone-view Benchmark** - Image-text-bounding box for vision-language.
- **Spatial Relation Matching** - Outperforming ALBEF and X-VLM.
- **31.2% Recall@10** - Significant improvement over baselines.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE IoT Journal 2022</div><img src='images/iot.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Multisensory Fusion, Haptic and Visual Feedback Based Teleoperation System Under IoT Framework**](https://ieeexplore.ieee.org/abstract/document/9758071)
<div style="display: inline">
    <a href="https://ieeexplore.ieee.org/abstract/document/9758071"> <strong>[Paper]</strong></a>
    <a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()" ><strong>[Abstract]</strong></a>
    <div class="abstract" style="overflow: hidden; display: none;">  
        <p> Developed a teleoperation system integrating multisensory fusion with haptic and visual feedback under IoT framework. Published in IEEE Internet of Things Journal with Impact Factor 8.2. </p>
    </div>
</div>

**Meng Chu**, Zhi Cui, Ailing Zhang, Jialei Yao, Chenyu Tang, Zhe Fu, Arokia Nathan, Shuo Gao

- **Impact Factor 8.2** - Published in IEEE Internet of Things Journal.
- **Multisensory Fusion** - Haptic and visual feedback integration.
- **IoT Framework** - Teleoperation system design.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/tafs.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis**](https://arxiv.org/abs/2512.19243)
<div style="display: inline">
    <a href="https://arxiv.org/abs/2512.19243"> <strong>[Paper]</strong></a>
    <a href="https://visiondirector.github.io/"> <strong>[Project]</strong></a>
    <a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()" ><strong>[Abstract]</strong></a>
    <div class="abstract" style="overflow: hidden; display: none;">  
        <p> Proposed LGBench, the first large-scale benchmark for long-horizon multi-goal image generation and editing, featuring 2,000 tasks and 29,000+ structured goals. Developed VisionDirector, a training-free VLM-driven closed-loop framework. Applied GRPO to reduce editing rounds by 26%. </p>
    </div>
</div>

**Meng Chu**, Senqiao Yang, Haoxuan Che, et al., Jiaya Jia

- **LGBench** - **2,000** tasks with **29,000+** structured goals.
- **Training-free** - VLM-driven closed-loop refinement.
- **26% Reduction** - Editing rounds reduced via GRPO.

</div>
</div>


# üíº Experience

- *2024.09 - 2025.04*: **Research Intern** @ OpenGVLab, Shanghai AI Laboratory
  - Mentors: Dr. Yi Wang, Prof. Limin Wang
  - Worked on VRBench for multi-step reasoning in long narrative videos.

- *2022.12 - 2024.07*: **Research Intern** @ NeXT++ Research Center, National University of Singapore
  - Mentors: Dr. Zhedong Zheng, Dr. Yicong Li
  - Developed GeoText-1652 benchmark and GraphVideoAgent.


# üéì Education

- *2024.09 - Present*: **Ph.D. in Computer Science and Engineering**
  - Hong Kong University of Science and Technology (HKUST)
  - Advisor: Prof. Jiaya Jia

- *2022.08 - 2024.06*: **M.Comp. in Artificial Intelligence** (Distinction)
  - National University of Singapore (NUS)
  - GPA: 4.27/5.0, Dean's List Fall 2023
  - Advisors: Prof. Tat-Seng Chua, Prof. Zhedong Zheng

- *2018.08 - 2022.06*: **B.Eng. in Instrumentation and Optoelectronic Engineering**
  - Beihang University (BUAA)
  - GPA: 3.6/4.0, Top 0.3% in Gaokao


# üéñ Honors and Awards 

- 2023 Dean's List, National University of Singapore
- 2024 M.Comp. with Distinction, National University of Singapore
- 2018 Top 0.3% in National College Entrance Examination (Gaokao)
